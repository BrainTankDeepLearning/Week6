{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gja4cLwbKbr",
        "outputId": "50d56ff2-de02-4aae-b871-b8e7ae13c752"
      },
      "source": [
        "!gdown --id 1JQPzJGIERV998BjtEcSSua3rMNhFpZkl\n",
        "!gdown --id 1hCtv60Vog1f6VWj45DCyVgmQ0_jn98IA\n",
        "!gdown --id 1qqL1uBarF3T1VcF0aN6jFO7tINgv1mrI\n",
        "!git clone https://github.com/BrainTankDeepLearning/Week6.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JQPzJGIERV998BjtEcSSua3rMNhFpZkl\n",
            "To: /content/Twitter_Data.csv\n",
            "100% 20.9M/20.9M [00:00<00:00, 65.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hCtv60Vog1f6VWj45DCyVgmQ0_jn98IA\n",
            "To: /content/glove.6B.50d.txt\n",
            "100% 171M/171M [00:01<00:00, 121MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qqL1uBarF3T1VcF0aN6jFO7tINgv1mrI\n",
            "To: /content/Reddit_Data.csv\n",
            "100% 6.89M/6.89M [00:00<00:00, 42.0MB/s]\n",
            "Cloning into 'Week6'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 13 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (13/13), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrlDQUlbY_OC"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import os\n",
        "import string\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "word2index = dict()\n",
        "index2word = dict()\n",
        "\n",
        "class GloveEmbeddings():\n",
        "  def __init__(self):\n",
        "    self.emb = dict()\n",
        "    with open('glove.6B.50d.txt','rt') as fi:\n",
        "        full_content = fi.read().strip().split('\\n')\n",
        "    for i in range(len(full_content)):\n",
        "        i_word = full_content[i].split(' ')[0]\n",
        "        i_embeddings = np.array([float(val) for val in full_content[i].split(' ')[1:]], dtype = np.float16)\n",
        "        self.emb[i_word] = i_embeddings\n",
        "\n",
        "  def get_emb(self, word):\n",
        "    if word not in self.emb:\n",
        "      self.emb[word] = np.random.uniform(0,1,50)\n",
        "    return self.emb[word]\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self, url = \"Reddit_Data.csv\"):\n",
        "    self.data = pd.read_csv(url, header = 0, names = [\"Text\", \"Label\"], skip_blank_lines = True, nrows = 5500)\n",
        "    self.data.dropna(subset = [\"Text\", \"Label\"], inplace=True)\n",
        "    self.data.astype({'Label': 'int8'}).dtypes\n",
        "    self.data = self.data.to_numpy()\n",
        "\n",
        "    print(len(self.data))\n",
        "\n",
        "    self.emb = GloveEmbeddings()\n",
        "\n",
        "    word2index[\"[EMPTY]\"] = 0\n",
        "    index2word[0] = \"[EMPTY]\"\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    data = self.data[idx]\n",
        "    text = data[0]\n",
        "    label = int(data[1])\n",
        "\n",
        "    sentence = re.sub(r'[^\\w\\s]', '', text)\n",
        "    sentence = sentence.replace(\"\\n\", \"\")\n",
        "    sentence = sentence.replace(\"\\t\", \"\")\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    words = sentence.split(\" \")\n",
        "    curr_one_hot = np.zeros([30, 50], dtype = np.float32)\n",
        "    curr_tokens = np.zeros(30, dtype = \"int32\")\n",
        "    \n",
        "    for word_num, word in enumerate(words):\n",
        "      if word_num >= 30:\n",
        "        break\n",
        "      curr_one_hot[word_num] = self.emb.get_emb(word)\n",
        "      if word not in word2index:\n",
        "        idx = len(word2index.keys())\n",
        "        word2index[word] = idx\n",
        "        index2word[idx] = word\n",
        "      curr_tokens[word_num] = idx\n",
        "\n",
        "    return {\"text\": curr_tokens, \"text_embedded\": curr_one_hot, \"label\": label}\n",
        "\n",
        "def get_dataloader(url = \"Reddit_Data.csv\"):\n",
        "  ds = TextDataset(url)\n",
        "    \n",
        "  train_ds, test_ds = torch.utils.data.random_split(ds, (5000, 490))\n",
        "\n",
        "  train_loader = DataLoader(train_ds, batch_size = 32, drop_last = True, shuffle = True)\n",
        "  test_loader = DataLoader(test_ds, batch_size = 32, drop_last = True, shuffle = True)\n",
        "\n",
        "  return train_loader, test_loader\n",
        "\n",
        "def tokens_to_text(tokens):\n",
        "  if len(tokens.shape) == 1:\n",
        "    tokens = tokens.unsqueeze(0)\n",
        "\n",
        "  out_list = []\n",
        "  for token_list in tokens:\n",
        "    word_list = \"\"\n",
        "    for token in token_list:\n",
        "      token = token.item()\n",
        "      if token == 0:\n",
        "        break\n",
        "      word = index2word[token]\n",
        "      word_list += word + \" \"\n",
        "    out_list.append(word_list)\n",
        "  \n",
        "  return out_list\n",
        "\n",
        "def softmax_loss(prediction, one_hot):\n",
        "  prediction = torch.log(prediction)\n",
        "\n",
        "  target = torch.empty(size = (len(one_hot), ), dtype = torch.long)\n",
        "  for i, row in enumerate(one_hot):\n",
        "    index = (row == 1).nonzero(as_tuple=True)[0]\n",
        "    target[i] = index.item()\n",
        "\n",
        "  loss = torch.nn.functional.nll_loss(prediction, target)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def create_ground_truth(label):\n",
        "  out = torch.zeros((32, 3))\n",
        "\n",
        "  for i in range(len(label)):\n",
        "    idx = label[i].item()\n",
        "\n",
        "    out[i][idx + 1] = 1.0\n",
        "\n",
        "  return out"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIJz4OCC1a2k",
        "outputId": "4c0a693f-55a5-4e73-fea0-ea6e34f765d1"
      },
      "source": [
        "class RNN_Cell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN_Cell, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x, hidden_state):\n",
        "        if len(x.shape) == 1:\n",
        "          x = x.unsqueeze(0)\n",
        "        combined = torch.cat((x, hidden_state), 1)\n",
        "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
        "        output = self.in2output(combined)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(RNN, self).__init__()\n",
        "    pass\n",
        "\n",
        "  def forward(self, embedded_sentence, sentence):\n",
        "    pass\n",
        "\n",
        "def train(model, train_dataset, optim):\n",
        "  pass\n",
        "\n",
        "def test(model, test_dataset):\n",
        "  total_correct = 0\n",
        "  for data in test_dataset:\n",
        "    text = data[\"text\"]\n",
        "    text_emb = data[\"text_embedded\"]\n",
        "    label = data[\"label\"]\n",
        "\n",
        "    prediction = model(text_emb, text)\n",
        "\n",
        "    highest_prediction = torch.argmax(prediction, dim = 1) - 1\n",
        "\n",
        "    n_correct = (highest_prediction == label).sum()\n",
        "\n",
        "    total_correct += n_correct.item()\n",
        "\n",
        "  print(f\"{total_correct / (len(test_dataset) * 32)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  train_ds, test_ds = get_dataloader(\"Reddit_Data.csv\")\n",
        "\n",
        "  model = RNN()\n",
        "  model.load_state_dict(torch.load(\"Week6/model_pretrained_rnn.state\", map_location=torch.device(\"cpu\")))\n",
        "\n",
        "  optim = torch.optim.Adam(model.parameters())\n",
        "\n",
        "  train(model, train_ds, optim)\n",
        "\n",
        "  test(model, test_ds)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5490\n",
            "0.7645833333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlwBztQf1Pvf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}