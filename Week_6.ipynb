{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gja4cLwbKbr",
        "outputId": "764aed59-7aeb-41f0-9f62-9961226347c5"
      },
      "source": [
        "!gdown --id 1JQPzJGIERV998BjtEcSSua3rMNhFpZkl\n",
        "!gdown --id 1hCtv60Vog1f6VWj45DCyVgmQ0_jn98IA\n",
        "!gdown --id 1qqL1uBarF3T1VcF0aN6jFO7tINgv1mrI\n",
        "!git clone https://github.com/BrainTankDeepLearning/Week6.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JQPzJGIERV998BjtEcSSua3rMNhFpZkl\n",
            "To: /content/Twitter_Data.csv\n",
            "100% 20.9M/20.9M [00:00<00:00, 93.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hCtv60Vog1f6VWj45DCyVgmQ0_jn98IA\n",
            "To: /content/glove.6B.50d.txt\n",
            "100% 171M/171M [00:01<00:00, 123MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qqL1uBarF3T1VcF0aN6jFO7tINgv1mrI\n",
            "To: /content/Reddit_Data.csv\n",
            "100% 6.89M/6.89M [00:00<00:00, 60.6MB/s]\n",
            "fatal: destination path 'Week6' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrlDQUlbY_OC"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import os\n",
        "import string\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "word2index = dict()\n",
        "index2word = dict()\n",
        "\n",
        "class GloveEmbeddings():\n",
        "  def __init__(self):\n",
        "    self.emb = dict()\n",
        "    with open('glove.6B.50d.txt','rt') as fi:\n",
        "        full_content = fi.read().strip().split('\\n')\n",
        "    for i in range(len(full_content)):\n",
        "        i_word = full_content[i].split(' ')[0]\n",
        "        i_embeddings = np.array([float(val) for val in full_content[i].split(' ')[1:]], dtype = np.float16)\n",
        "        self.emb[i_word] = i_embeddings\n",
        "\n",
        "  def get_emb(self, word):\n",
        "    if word not in self.emb:\n",
        "      self.emb[word] = np.random.uniform(0,1,50)\n",
        "    return self.emb[word]\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self, url = \"Reddit_Data.csv\"):\n",
        "    self.data = pd.read_csv(url, header = 0, names = [\"Text\", \"Label\"], skip_blank_lines = True, nrows = 5500)\n",
        "    self.data.dropna(subset = [\"Text\", \"Label\"], inplace=True)\n",
        "    self.data.astype({'Label': 'int8'}).dtypes\n",
        "    self.data = self.data.to_numpy()\n",
        "\n",
        "    self.emb = GloveEmbeddings()\n",
        "\n",
        "    word2index[\"[EMPTY]\"] = 0\n",
        "    index2word[0] = \"[EMPTY]\"\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    data = self.data[idx]\n",
        "    text = data[0]\n",
        "    label = int(data[1])\n",
        "\n",
        "    sentence = re.sub(r'[^\\w\\s]', '', text)\n",
        "    sentence = sentence.replace(\"\\n\", \"\")\n",
        "    sentence = sentence.replace(\"\\t\", \"\")\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    words = sentence.split(\" \")\n",
        "\n",
        "    curr_one_hot = np.zeros([30, 50], dtype = np.float32)\n",
        "    curr_tokens = np.zeros(30, dtype = \"int32\")\n",
        "    \n",
        "    for word_num, word in enumerate(words):\n",
        "      if word_num >= 30:\n",
        "        break\n",
        "      curr_one_hot[word_num] = self.emb.get_emb(word)\n",
        "      if word not in word2index:\n",
        "        idx = len(word2index.keys())\n",
        "        word2index[word] = idx\n",
        "        index2word[idx] = word\n",
        "      idx = word2index[word]\n",
        "      curr_tokens[word_num] = idx\n",
        "\n",
        "    return {\"text\": curr_tokens, \"text_embedded\": curr_one_hot, \"label\": label}\n",
        "\n",
        "def get_dataloader(url = \"Reddit_Data.csv\"):\n",
        "  ds = TextDataset(url)\n",
        "    \n",
        "  train_ds, test_ds = torch.utils.data.random_split(ds, (5000, 490))\n",
        "\n",
        "  train_loader = DataLoader(train_ds, batch_size = 32, drop_last = True, shuffle = True)\n",
        "  test_loader = DataLoader(test_ds, batch_size = 32, drop_last = True, shuffle = True)\n",
        "\n",
        "  return train_loader, test_loader\n",
        "\n",
        "def tokens_to_text(tokens):\n",
        "  if len(tokens.shape) == 1:\n",
        "    tokens = tokens.unsqueeze(0)\n",
        "\n",
        "  out_list = []\n",
        "  for token_list in tokens:\n",
        "    word_list = \"\"\n",
        "    for token in token_list:\n",
        "      token = token.item()\n",
        "      if token == 0:\n",
        "        break\n",
        "      word = index2word[token]\n",
        "      word_list += word + \" \"\n",
        "    out_list.append(word_list)\n",
        "  \n",
        "  return out_list\n",
        "\n",
        "def softmax_loss(prediction, one_hot):\n",
        "  prediction = torch.log(prediction)\n",
        "\n",
        "  target = torch.empty(size = (len(one_hot), ), dtype = torch.long)\n",
        "  for i, row in enumerate(one_hot):\n",
        "    index = (row == 1).nonzero(as_tuple=True)[0]\n",
        "    target[i] = index.item()\n",
        "\n",
        "  loss = torch.nn.functional.nll_loss(prediction, target)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def create_ground_truth(label):\n",
        "  out = torch.zeros((32, 3))\n",
        "\n",
        "  for i in range(len(label)):\n",
        "    idx = label[i].item()\n",
        "\n",
        "    out[i][idx + 1] = 1.0\n",
        "\n",
        "  return out"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "LIJz4OCC1a2k",
        "outputId": "6686238d-9eac-48ab-aa16-a4fcb99c3944"
      },
      "source": [
        "class RNN_Cell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN_Cell, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x, hidden_state):\n",
        "        if len(x.shape) == 1:\n",
        "          x = x.unsqueeze(0)\n",
        "        combined = torch.cat((x, hidden_state), 1)\n",
        "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
        "        output = self.in2output(combined)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(RNN, self).__init__()\n",
        "    pass\n",
        "\n",
        "  def forward(self, embedded_sentence, sentence):\n",
        "    pass\n",
        "\n",
        "def train(model, train_dataset, optim):\n",
        "  pass\n",
        "\n",
        "def test(model, test_dataset):\n",
        "  total_correct = 0\n",
        "  for data in test_dataset:\n",
        "    text = data[\"text\"]\n",
        "    text_emb = data[\"text_embedded\"]\n",
        "    label = data[\"label\"]\n",
        "\n",
        "    prediction = model(text_emb, text)\n",
        "\n",
        "    highest_prediction = torch.argmax(prediction, dim = 1) - 1\n",
        "\n",
        "    n_correct = (highest_prediction == label).sum()\n",
        "\n",
        "    total_correct += n_correct.item()\n",
        "\n",
        "  print(f\"{total_correct / (len(test_dataset) * 32)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  train_ds, test_ds = get_dataloader(\"Reddit_Data.csv\")\n",
        "\n",
        "  model = RNN()\n",
        "  #model.load_state_dict(torch.load(\"Week6/model_pretrained_rnn.state\", map_location=torch.device(\"cpu\")))\n",
        "\n",
        "  optim = torch.optim.Adam(model.parameters())\n",
        "\n",
        "  train(model, train_ds, optim)\n",
        "\n",
        "  test(model, test_ds)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-735573aee11e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;31m#model.load_state_dict(torch.load(\"Week6/model_pretrained_rnn.state\", map_location=torch.device(\"cpu\")))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     72\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     73\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
          ]
        }
      ]
    }
  ]
}